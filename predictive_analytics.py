"""
Predictive Analytics Engine - Phase 4.3 Module

This module provides predictive analytics capabilities including timeline gap prediction,
sender behavior forecasting, import success estimation, and business intelligence insights.
Builds on Phase 3 analytics data and Phase 4 ML capabilities.
"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import logging
from typing import Dict, List, Tuple, Any, Optional
import statistics
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import math
import warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TimelinePrediction:
    """Timeline gap prediction with confidence intervals"""
    contact_email: str
    predicted_gap_start: datetime
    predicted_gap_end: datetime
    gap_type: str  # 'vacation', 'business_trip', 'seasonal', 'concerning'
    confidence: float
    severity: int  # 1-5 scale
    early_warning_days: int
    recommendation: str

@dataclass
class SenderForecast:
    """Sender behavior forecast"""
    sender_email: str
    current_activity_level: str  # 'high', 'medium', 'low', 'declining', 'inactive'
    predicted_activity_trend: str  # 'increasing', 'stable', 'declining', 'ending'
    next_contact_probability: float
    predicted_next_contact: Optional[datetime]
    predicted_frequency_change: float  # percentage change
    confidence: float
    business_impact: str

@dataclass
class BusinessInsight:
    """Business intelligence insight"""
    insight_type: str
    title: str
    description: str
    impact_level: str  # 'high', 'medium', 'low'
    actionable: bool
    recommendation: str
    supporting_data: Dict[str, Any]
    confidence: float

@dataclass
class ImportPrediction:
    """Import success and performance prediction"""
    email_count: int
    predicted_success_rate: float
    predicted_duration: float
    predicted_errors: int
    risk_factors: List[str]
    confidence: float
    recommended_actions: List[str]

class PredictiveAnalytics:
    """
    Advanced Predictive Analytics Engine for email import intelligence.
    
    Capabilities:
    - Timeline gap prediction with early warning system
    - Sender behavior forecasting and trend analysis
    - Import success probability estimation
    - Business intelligence insight generation
    - Capacity planning and scaling recommendations
    - Trend analysis and pattern forecasting
    """
    
    def __init__(self, analytics_db_path: str = "analytics.db"):
        """Initialize the Predictive Analytics Engine"""
        self.analytics_db_path = analytics_db_path
        self.predictions_db_path = "predictions.db"
        
        # Prediction models and data
        self.timeline_predictions = []
        self.sender_forecasts = {}
        self.business_insights = []
        self.import_predictions = []
        
        # Historical data cache
        self.historical_data = None
        self.sender_patterns = {}
        self.timeline_patterns = {}
        
        # Prediction parameters
        self.prediction_window_days = 90
        self.confidence_threshold = 0.7
        self.early_warning_days = 7
        
        # Initialize database
        self.init_predictions_database()
        
        logger.info("Predictive Analytics Engine initialized successfully")
    
    def init_predictions_database(self):
        """Initialize predictions tracking database"""
        try:
            conn = sqlite3.connect(self.predictions_db_path)
            cursor = conn.cursor()
            
            # Timeline predictions table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS timeline_predictions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    contact_email TEXT,
                    predicted_gap_start TEXT,
                    predicted_gap_end TEXT,
                    gap_type TEXT,
                    confidence REAL,
                    severity INTEGER,
                    early_warning_days INTEGER,
                    recommendation TEXT,
                    actual_gap_start TEXT DEFAULT NULL,
                    actual_gap_end TEXT DEFAULT NULL,
                    prediction_accuracy REAL DEFAULT NULL
                )
            """)
            
            # Sender forecasts table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS sender_forecasts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    sender_email TEXT,
                    current_activity_level TEXT,
                    predicted_activity_trend TEXT,
                    next_contact_probability REAL,
                    predicted_next_contact TEXT,
                    predicted_frequency_change REAL,
                    confidence REAL,
                    business_impact TEXT,
                    validation_date TEXT DEFAULT NULL,
                    actual_outcome TEXT DEFAULT NULL,
                    forecast_accuracy REAL DEFAULT NULL
                )
            """)
            
            # Business insights table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS business_insights (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    insight_type TEXT,
                    title TEXT,
                    description TEXT,
                    impact_level TEXT,
                    actionable BOOLEAN,
                    recommendation TEXT,
                    supporting_data TEXT,
                    confidence REAL,
                    acted_upon BOOLEAN DEFAULT FALSE,
                    outcome_measured BOOLEAN DEFAULT FALSE
                )
            """)
            
            # Import predictions table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS import_predictions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    email_count INTEGER,
                    predicted_success_rate REAL,
                    predicted_duration REAL,
                    predicted_errors INTEGER,
                    risk_factors TEXT,
                    confidence REAL,
                    recommended_actions TEXT,
                    actual_success_rate REAL DEFAULT NULL,
                    actual_duration REAL DEFAULT NULL,
                    actual_errors INTEGER DEFAULT NULL,
                    prediction_accuracy REAL DEFAULT NULL
                )
            """)
            
            conn.commit()
            conn.close()
            
            logger.info("Predictions database initialized successfully")
            
        except Exception as e:
            logger.error(f"Error initializing predictions database: {e}")
    
    def load_historical_data(self) -> bool:
        """Load and prepare historical data for predictions"""
        try:
            conn = sqlite3.connect(self.analytics_db_path)
            
            # Load sender analytics
            sender_query = """
            SELECT email_address, category, importance_score, avg_response_time,
                   total_emails, first_contact, last_contact, communication_pattern
            FROM sender_analytics
            WHERE total_emails > 0
            """
            
            sender_df = pd.read_sql_query(sender_query, conn)
            
            # Load import analytics
            import_query = """
            SELECT session_id, start_time, end_time, total_emails, successful_emails,
                   failed_emails, success_rate, processing_speed
            FROM import_analytics
            WHERE total_emails > 0
            """
            
            import_df = pd.read_sql_query(import_query, conn)
            
            conn.close()
            
            self.historical_data = {
                'senders': sender_df,
                'imports': import_df
            }
            
            # Analyze patterns
            self._analyze_sender_patterns()
            self._analyze_timeline_patterns()
            
            logger.info(f"Loaded historical data: {len(sender_df)} senders, {len(import_df)} imports")
            return True
            
        except Exception as e:
            logger.error(f"Error loading historical data: {e}")
            return False
    
    def predict_timeline_gaps(self, contact_email: str, days_ahead: int = 30) -> List[TimelinePrediction]:
        """Predict potential timeline gaps for a specific contact"""
        try:
            predictions = []
            
            if not self.historical_data or contact_email not in self.sender_patterns:
                return predictions
            
            patterns = self.sender_patterns[contact_email]
            
            # Analyze historical communication frequency
            avg_gap_days = patterns.get('avg_gap_days', 7)
            gap_variance = patterns.get('gap_variance', 3)
            seasonal_patterns = patterns.get('seasonal_patterns', {})
            last_contact = patterns.get('last_contact')
            
            if not last_contact:
                return predictions
            
            last_contact_date = pd.to_datetime(last_contact)
            current_date = datetime.now()
            
            # Predict based on frequency patterns
            days_since_last = (current_date - last_contact_date).days
            
            # Check if already in a gap
            if days_since_last > avg_gap_days * 2:
                # Currently in a concerning gap
                gap_end_prediction = current_date + timedelta(days=avg_gap_days)
                confidence = min(0.9, days_since_last / (avg_gap_days * 3))
                
                prediction = TimelinePrediction(
                    contact_email=contact_email,
                    predicted_gap_start=last_contact_date,
                    predicted_gap_end=gap_end_prediction,
                    gap_type="concerning_ongoing",
                    confidence=confidence,
                    severity=4 if days_since_last > avg_gap_days * 4 else 3,
                    early_warning_days=0,  # Already happening
                    recommendation=f"Contact has been inactive for {days_since_last} days. Immediate follow-up recommended."
                )
                predictions.append(prediction)
            
            # Predict future gaps
            prediction_start = max(current_date, last_contact_date + timedelta(days=avg_gap_days))
            
            for i in range(days_ahead // int(avg_gap_days) + 1):
                gap_start = prediction_start + timedelta(days=i * avg_gap_days)
                
                if gap_start > current_date + timedelta(days=days_ahead):
                    break
                
                # Check for seasonal patterns
                gap_type = self._classify_predicted_gap(gap_start, seasonal_patterns)
                severity = self._calculate_gap_severity(gap_type, avg_gap_days)
                confidence = self._calculate_prediction_confidence(patterns, gap_start)
                
                if confidence > self.confidence_threshold:
                    gap_duration = self._estimate_gap_duration(gap_type, avg_gap_days, gap_variance)
                    gap_end = gap_start + timedelta(days=gap_duration)
                    
                    early_warning = max(0, (gap_start - current_date).days - self.early_warning_days)
                    
                    prediction = TimelinePrediction(
                        contact_email=contact_email,
                        predicted_gap_start=gap_start,
                        predicted_gap_end=gap_end,
                        gap_type=gap_type,
                        confidence=confidence,
                        severity=severity,
                        early_warning_days=early_warning,
                        recommendation=self._generate_timeline_recommendation(gap_type, gap_duration)
                    )
                    predictions.append(prediction)
            
            # Store predictions
            for pred in predictions:
                self._store_timeline_prediction(pred)
            
            self.timeline_predictions.extend(predictions)
            return predictions
            
        except Exception as e:
            logger.error(f"Error predicting timeline gaps for {contact_email}: {e}")
            return []
    
    def forecast_sender_behavior(self, sender_email: str, forecast_days: int = 60) -> SenderForecast:
        """Forecast sender behavior and activity trends"""
        try:
            if not self.historical_data or sender_email not in self.sender_patterns:
                return SenderForecast(
                    sender_email=sender_email,
                    current_activity_level="unknown",
                    predicted_activity_trend="unknown",
                    next_contact_probability=0.5,
                    predicted_next_contact=None,
                    predicted_frequency_change=0.0,
                    confidence=0.0,
                    business_impact="unknown"
                )
            
            patterns = self.sender_patterns[sender_email]
            
            # Analyze current activity level
            current_activity = self._assess_current_activity(patterns)
            
            # Predict activity trend
            trend = self._predict_activity_trend(patterns, forecast_days)
            
            # Calculate next contact probability
            next_prob = self._calculate_next_contact_probability(patterns, forecast_days)
            
            # Predict next contact date
            next_contact = self._predict_next_contact_date(patterns) if next_prob > 0.3 else None
            
            # Predict frequency change
            freq_change = self._predict_frequency_change(patterns)
            
            # Calculate confidence
            confidence = self._calculate_forecast_confidence(patterns)
            
            # Assess business impact
            business_impact = self._assess_business_impact(patterns, trend)
            
            forecast = SenderForecast(
                sender_email=sender_email,
                current_activity_level=current_activity,
                predicted_activity_trend=trend,
                next_contact_probability=next_prob,
                predicted_next_contact=next_contact,
                predicted_frequency_change=freq_change,
                confidence=confidence,
                business_impact=business_impact
            )
            
            # Store forecast
            self._store_sender_forecast(forecast)
            self.sender_forecasts[sender_email] = forecast
            
            return forecast
            
        except Exception as e:
            logger.error(f"Error forecasting sender behavior for {sender_email}: {e}")
            return SenderForecast(
                sender_email=sender_email,
                current_activity_level="error",
                predicted_activity_trend="unknown",
                next_contact_probability=0.0,
                predicted_next_contact=None,
                predicted_frequency_change=0.0,
                confidence=0.0,
                business_impact="unknown"
            )
    
    def predict_import_success(self, email_count: int, import_characteristics: Dict[str, Any] = None) -> ImportPrediction:
        """Predict import success probability and potential issues"""
        try:
            if not self.historical_data:
                self.load_historical_data()
            
            # Analyze historical import performance
            import_history = self.historical_data.get('imports', pd.DataFrame())
            
            if len(import_history) == 0:
                # No historical data, provide conservative estimates
                return ImportPrediction(
                    email_count=email_count,
                    predicted_success_rate=0.85,
                    predicted_duration=email_count / 500.0 * 60,  # Assume 500 emails/min
                    predicted_errors=int(email_count * 0.15),
                    risk_factors=["No historical data available"],
                    confidence=0.3,
                    recommended_actions=["Start with smaller batch size", "Monitor closely"]
                )
            
            # Calculate success rate based on similar imports
            similar_imports = import_history[
                (import_history['total_emails'] >= email_count * 0.5) &
                (import_history['total_emails'] <= email_count * 2.0)
            ]
            
            if len(similar_imports) > 0:
                avg_success_rate = similar_imports['success_rate'].mean()
                success_variance = similar_imports['success_rate'].std()
                avg_speed = similar_imports['processing_speed'].mean()
            else:
                avg_success_rate = import_history['success_rate'].mean()
                success_variance = import_history['success_rate'].std()
                avg_speed = import_history['processing_speed'].mean()
            
            # Predict success rate with confidence interval
            predicted_success_rate = max(0.1, min(1.0, avg_success_rate))
            
            # Predict duration
            if avg_speed > 0:
                predicted_duration = (email_count / avg_speed) * 60  # Convert to seconds
            else:
                predicted_duration = email_count / 500.0 * 60
            
            # Predict errors
            predicted_errors = int(email_count * (1 - predicted_success_rate))
            
            # Identify risk factors
            risk_factors = []
            if email_count > 1000:
                risk_factors.append("Large batch size increases complexity")
            if predicted_success_rate < 0.9:
                risk_factors.append("Historical success rate below optimal")
            if import_characteristics:
                if import_characteristics.get('has_large_attachments', False):
                    risk_factors.append("Large attachments may slow processing")
                if import_characteristics.get('complex_contacts', False):
                    risk_factors.append("Complex contact structures may cause errors")
            
            # Calculate confidence
            confidence = max(0.1, min(0.95, 1.0 - (success_variance if success_variance else 0.1)))
            if len(similar_imports) == 0:
                confidence *= 0.7  # Lower confidence without similar imports
            
            # Generate recommendations
            recommended_actions = []
            if predicted_success_rate < 0.85:
                recommended_actions.append("Pre-validate email data")
            if email_count > 500:
                recommended_actions.append("Use batch processing")
            if predicted_duration > 300:  # 5 minutes
                recommended_actions.append("Schedule during low-usage periods")
            
            prediction = ImportPrediction(
                email_count=email_count,
                predicted_success_rate=predicted_success_rate,
                predicted_duration=predicted_duration,
                predicted_errors=predicted_errors,
                risk_factors=risk_factors,
                confidence=confidence,
                recommended_actions=recommended_actions
            )
            
            # Store prediction
            self._store_import_prediction(prediction)
            self.import_predictions.append(prediction)
            
            return prediction
            
        except Exception as e:
            logger.error(f"Error predicting import success: {e}")
            return ImportPrediction(
                email_count=email_count,
                predicted_success_rate=0.8,
                predicted_duration=email_count / 500.0 * 60,
                predicted_errors=int(email_count * 0.2),
                risk_factors=["Prediction error occurred"],
                confidence=0.1,
                recommended_actions=["Manual validation recommended"]
            )
    
    def generate_business_insights(self) -> List[BusinessInsight]:
        """Generate business intelligence insights from predictive analysis"""
        try:
            insights = []
            
            if not self.historical_data:
                self.load_historical_data()
            
            # Communication trend insights
            insights.extend(self._analyze_communication_trends())
            
            # Sender relationship insights
            insights.extend(self._analyze_sender_relationships())
            
            # Timeline efficiency insights
            insights.extend(self._analyze_timeline_efficiency())
            
            # Import performance insights
            insights.extend(self._analyze_import_performance())
            
            # Risk and opportunity insights
            insights.extend(self._identify_risks_and_opportunities())
            
            # Store insights
            for insight in insights:
                self._store_business_insight(insight)
            
            self.business_insights.extend(insights)
            
            logger.info(f"Generated {len(insights)} business insights")
            return insights
            
        except Exception as e:
            logger.error(f"Error generating business insights: {e}")
            return []
    
    def _analyze_sender_patterns(self):
        """Analyze sender patterns from historical data"""
        if not self.historical_data or 'senders' not in self.historical_data:
            return
        
        senders_df = self.historical_data['senders']
        
        for _, sender in senders_df.iterrows():
            email = sender['email_address']
            
            # Calculate communication patterns
            first_contact = pd.to_datetime(sender['first_contact']) if sender['first_contact'] else None
            last_contact = pd.to_datetime(sender['last_contact']) if sender['last_contact'] else None
            
            if first_contact and last_contact:
                total_days = (last_contact - first_contact).days
                avg_gap_days = total_days / max(sender['total_emails'] - 1, 1)
                
                # Estimate variance (simplified)
                gap_variance = max(1, avg_gap_days * 0.3)
                
                # Seasonal patterns (simplified)
                seasonal_patterns = {
                    'summer': 0.8,  # 20% less activity in summer
                    'winter': 1.1,  # 10% more activity in winter
                    'holidays': 0.5  # 50% less during holidays
                }
                
                self.sender_patterns[email] = {
                    'avg_gap_days': avg_gap_days,
                    'gap_variance': gap_variance,
                    'seasonal_patterns': seasonal_patterns,
                    'last_contact': last_contact,
                    'total_emails': sender['total_emails'],
                    'importance_score': sender['importance_score'],
                    'category': sender['category']
                }
    
    def _analyze_timeline_patterns(self):
        """Analyze timeline patterns for prediction"""
        # Simplified timeline pattern analysis
        self.timeline_patterns = {
            'average_gap_days': 7,
            'seasonal_variance': 0.2,
            'holiday_impact': 0.5
        }
    
    def _classify_predicted_gap(self, gap_start: datetime, seasonal_patterns: Dict) -> str:
        """Classify the type of predicted gap"""
        month = gap_start.month
        
        # Holiday periods
        if month == 12 or month == 1:
            return "holiday"
        elif month in [7, 8]:  # Summer
            return "vacation"
        elif gap_start.weekday() >= 5:  # Weekend
            return "weekend"
        else:
            return "business"
    
    def _calculate_gap_severity(self, gap_type: str, avg_gap_days: float) -> int:
        """Calculate gap severity on 1-5 scale"""
        severity_map = {
            "weekend": 1,
            "business": 2,
            "vacation": 3,
            "holiday": 2,
            "concerning_ongoing": 5
        }
        
        base_severity = severity_map.get(gap_type, 2)
        
        # Adjust based on gap length
        if avg_gap_days > 30:
            base_severity = min(5, base_severity + 2)
        elif avg_gap_days > 14:
            base_severity = min(5, base_severity + 1)
        
        return base_severity
    
    def _calculate_prediction_confidence(self, patterns: Dict, gap_start: datetime) -> float:
        """Calculate confidence for timeline prediction"""
        base_confidence = 0.7
        
        # Adjust for data quality
        if patterns['total_emails'] > 20:
            base_confidence += 0.2
        elif patterns['total_emails'] < 5:
            base_confidence -= 0.3
        
        # Adjust for time distance
        days_ahead = (gap_start - datetime.now()).days
        if days_ahead > 30:
            base_confidence -= 0.1
        
        return max(0.1, min(0.95, base_confidence))
    
    def _estimate_gap_duration(self, gap_type: str, avg_gap: float, variance: float) -> int:
        """Estimate duration of predicted gap"""
        duration_map = {
            "weekend": 2,
            "business": max(1, int(avg_gap)),
            "vacation": max(7, int(avg_gap * 1.5)),
            "holiday": max(3, int(avg_gap)),
            "concerning_ongoing": max(7, int(avg_gap * 2))
        }
        
        base_duration = duration_map.get(gap_type, int(avg_gap))
        return max(1, base_duration)
    
    def _generate_timeline_recommendation(self, gap_type: str, duration: int) -> str:
        """Generate recommendation for predicted timeline gap"""
        recommendations = {
            "weekend": f"Normal {duration}-day weekend gap expected",
            "business": f"Regular {duration}-day business gap predicted",
            "vacation": f"Potential {duration}-day vacation period - plan accordingly",
            "holiday": f"Holiday period gap of {duration} days expected",
            "concerning_ongoing": f"Urgent: {duration}-day inactive period requires immediate attention"
        }
        
        return recommendations.get(gap_type, f"Predicted {duration}-day communication gap")
    
    # Additional helper methods for sender behavior forecasting
    def _assess_current_activity(self, patterns: Dict) -> str:
        """Assess current activity level of sender"""
        last_contact = patterns.get('last_contact')
        avg_gap = patterns.get('avg_gap_days', 7)
        
        if not last_contact:
            return "unknown"
        
        days_since = (datetime.now() - last_contact).days
        
        if days_since <= avg_gap * 0.5:
            return "high"
        elif days_since <= avg_gap:
            return "medium"
        elif days_since <= avg_gap * 2:
            return "low"
        elif days_since <= avg_gap * 4:
            return "declining"
        else:
            return "inactive"
    
    def _predict_activity_trend(self, patterns: Dict, forecast_days: int) -> str:
        """Predict activity trend for sender"""
        current_activity = self._assess_current_activity(patterns)
        
        if current_activity == "inactive":
            return "ending"
        elif current_activity == "declining":
            return "declining"
        elif current_activity in ["high", "medium"]:
            return "stable"
        else:
            return "unknown"
    
    def _calculate_next_contact_probability(self, patterns: Dict, forecast_days: int) -> float:
        """Calculate probability of next contact"""
        avg_gap = patterns.get('avg_gap_days', 7)
        last_contact = patterns.get('last_contact')
        
        if not last_contact:
            return 0.3
        
        days_since = (datetime.now() - last_contact).days
        
        # Probability increases as time approaches average gap
        if days_since >= avg_gap:
            return min(0.9, 0.5 + (days_since / avg_gap) * 0.3)
        else:
            return max(0.1, 0.5 - ((avg_gap - days_since) / avg_gap) * 0.3)
    
    def _predict_next_contact_date(self, patterns: Dict) -> Optional[datetime]:
        """Predict next contact date"""
        last_contact = patterns.get('last_contact')
        avg_gap = patterns.get('avg_gap_days', 7)
        
        if not last_contact:
            return None
        
        # Simple prediction: last contact + average gap
        predicted_date = last_contact + timedelta(days=avg_gap)
        
        # Don't predict in the past
        if predicted_date < datetime.now():
            predicted_date = datetime.now() + timedelta(days=avg_gap)
        
        return predicted_date
    
    def _predict_frequency_change(self, patterns: Dict) -> float:
        """Predict change in communication frequency (percentage)"""
        current_activity = self._assess_current_activity(patterns)
        
        change_map = {
            "high": 0.1,      # 10% increase
            "medium": 0.0,    # No change
            "low": -0.2,      # 20% decrease
            "declining": -0.4, # 40% decrease
            "inactive": -0.8   # 80% decrease
        }
        
        return change_map.get(current_activity, 0.0)
    
    def _calculate_forecast_confidence(self, patterns: Dict) -> float:
        """Calculate confidence in sender behavior forecast"""
        confidence = 0.6  # Base confidence
        
        # Adjust for data quantity
        emails = patterns.get('total_emails', 0)
        if emails > 50:
            confidence += 0.3
        elif emails > 20:
            confidence += 0.2
        elif emails < 5:
            confidence -= 0.4
        
        # Adjust for recency
        last_contact = patterns.get('last_contact')
        if last_contact:
            days_ago = (datetime.now() - last_contact).days
            if days_ago > 90:
                confidence -= 0.2
        
        return max(0.1, min(0.95, confidence))
    
    def _assess_business_impact(self, patterns: Dict, trend: str) -> str:
        """Assess business impact of sender behavior"""
        importance = patterns.get('importance_score', 0.5)
        
        if importance > 0.8:
            if trend in ["declining", "ending"]:
                return "high_risk"
            else:
                return "high_value"
        elif importance > 0.5:
            if trend == "ending":
                return "medium_risk"
            else:
                return "medium_value"
        else:
            return "low_impact"
    
    # Business insights generation methods
    def _analyze_communication_trends(self) -> List[BusinessInsight]:
        """Analyze overall communication trends"""
        insights = []
        
        # Placeholder for trend analysis
        insights.append(BusinessInsight(
            insight_type="communication_trend",
            title="Communication Pattern Analysis",
            description="Overall communication trends are stable with predictable patterns",
            impact_level="medium",
            actionable=True,
            recommendation="Continue monitoring for pattern changes",
            supporting_data={"trend": "stable"},
            confidence=0.8
        ))
        
        return insights
    
    def _analyze_sender_relationships(self) -> List[BusinessInsight]:
        """Analyze sender relationship patterns"""
        insights = []
        
        if self.sender_patterns:
            high_value_senders = [email for email, patterns in self.sender_patterns.items() 
                                if patterns.get('importance_score', 0) > 0.7]
            
            if high_value_senders:
                insights.append(BusinessInsight(
                    insight_type="sender_relationships",
                    title="High-Value Sender Identification",
                    description=f"Identified {len(high_value_senders)} high-value communication relationships",
                    impact_level="high",
                    actionable=True,
                    recommendation="Prioritize maintenance of these key relationships",
                    supporting_data={"high_value_count": len(high_value_senders)},
                    confidence=0.9
                ))
        
        return insights
    
    def _analyze_timeline_efficiency(self) -> List[BusinessInsight]:
        """Analyze timeline efficiency patterns"""
        insights = []
        
        insights.append(BusinessInsight(
            insight_type="timeline_efficiency",
            title="Timeline Efficiency Assessment",
            description="Communication timelines show opportunities for optimization",
            impact_level="medium",
            actionable=True,
            recommendation="Implement proactive follow-up scheduling",
            supporting_data={"efficiency_score": 0.75},
            confidence=0.7
        ))
        
        return insights
    
    def _analyze_import_performance(self) -> List[BusinessInsight]:
        """Analyze import performance patterns"""
        insights = []
        
        if self.historical_data and 'imports' in self.historical_data:
            imports_df = self.historical_data['imports']
            if len(imports_df) > 0:
                avg_success_rate = imports_df['success_rate'].mean()
                
                insights.append(BusinessInsight(
                    insight_type="import_performance",
                    title="Import Performance Analysis",
                    description=f"Import success rate averaging {avg_success_rate:.1%}",
                    impact_level="high" if avg_success_rate > 0.9 else "medium",
                    actionable=True,
                    recommendation="Maintain current performance levels" if avg_success_rate > 0.9 
                                 else "Investigate and improve import reliability",
                    supporting_data={"success_rate": avg_success_rate},
                    confidence=0.9
                ))
        
        return insights
    
    def _identify_risks_and_opportunities(self) -> List[BusinessInsight]:
        """Identify risks and opportunities from predictions"""
        insights = []
        
        # Risk identification
        inactive_senders = [email for email, patterns in self.sender_patterns.items() 
                          if self._assess_current_activity(patterns) == "inactive"]
        
        if inactive_senders:
            insights.append(BusinessInsight(
                insight_type="risk_identification",
                title="Inactive Sender Alert",
                description=f"{len(inactive_senders)} senders have become inactive",
                impact_level="medium",
                actionable=True,
                recommendation="Implement re-engagement strategy for inactive contacts",
                supporting_data={"inactive_count": len(inactive_senders)},
                confidence=0.8
            ))
        
        return insights
    
    # Database storage methods
    def _store_timeline_prediction(self, prediction: TimelinePrediction):
        """Store timeline prediction in database"""
        try:
            conn = sqlite3.connect(self.predictions_db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO timeline_predictions 
                (timestamp, contact_email, predicted_gap_start, predicted_gap_end,
                 gap_type, confidence, severity, early_warning_days, recommendation)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                prediction.contact_email,
                prediction.predicted_gap_start.isoformat(),
                prediction.predicted_gap_end.isoformat(),
                prediction.gap_type,
                prediction.confidence,
                prediction.severity,
                prediction.early_warning_days,
                prediction.recommendation
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Error storing timeline prediction: {e}")
    
    def _store_sender_forecast(self, forecast: SenderForecast):
        """Store sender forecast in database"""
        try:
            conn = sqlite3.connect(self.predictions_db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO sender_forecasts 
                (timestamp, sender_email, current_activity_level, predicted_activity_trend,
                 next_contact_probability, predicted_next_contact, predicted_frequency_change,
                 confidence, business_impact)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                forecast.sender_email,
                forecast.current_activity_level,
                forecast.predicted_activity_trend,
                forecast.next_contact_probability,
                forecast.predicted_next_contact.isoformat() if forecast.predicted_next_contact else None,
                forecast.predicted_frequency_change,
                forecast.confidence,
                forecast.business_impact
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Error storing sender forecast: {e}")
    
    def _store_business_insight(self, insight: BusinessInsight):
        """Store business insight in database"""
        try:
            conn = sqlite3.connect(self.predictions_db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO business_insights 
                (timestamp, insight_type, title, description, impact_level,
                 actionable, recommendation, supporting_data, confidence)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                insight.insight_type,
                insight.title,
                insight.description,
                insight.impact_level,
                insight.actionable,
                insight.recommendation,
                json.dumps(insight.supporting_data),
                insight.confidence
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Error storing business insight: {e}")
    
    def _store_import_prediction(self, prediction: ImportPrediction):
        """Store import prediction in database"""
        try:
            conn = sqlite3.connect(self.predictions_db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO import_predictions 
                (timestamp, email_count, predicted_success_rate, predicted_duration,
                 predicted_errors, risk_factors, confidence, recommended_actions)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                prediction.email_count,
                prediction.predicted_success_rate,
                prediction.predicted_duration,
                prediction.predicted_errors,
                json.dumps(prediction.risk_factors),
                prediction.confidence,
                json.dumps(prediction.recommended_actions)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Error storing import prediction: {e}")
    
    def get_predictive_summary(self) -> Dict[str, Any]:
        """Get comprehensive predictive analytics summary"""
        try:
            summary = {
                'timestamp': datetime.now().isoformat(),
                'timeline_predictions': {
                    'total_predictions': len(self.timeline_predictions),
                    'high_severity_gaps': len([p for p in self.timeline_predictions if p.severity >= 4]),
                    'early_warnings_active': len([p for p in self.timeline_predictions 
                                                if p.early_warning_days <= 7 and p.early_warning_days > 0])
                },
                'sender_forecasts': {
                    'total_forecasts': len(self.sender_forecasts),
                    'declining_senders': len([f for f in self.sender_forecasts.values() 
                                            if f.predicted_activity_trend == 'declining']),
                    'high_risk_relationships': len([f for f in self.sender_forecasts.values() 
                                                  if f.business_impact == 'high_risk'])
                },
                'business_insights': {
                    'total_insights': len(self.business_insights),
                    'high_impact_insights': len([i for i in self.business_insights if i.impact_level == 'high']),
                    'actionable_insights': len([i for i in self.business_insights if i.actionable])
                },
                'import_predictions': {
                    'total_predictions': len(self.import_predictions),
                    'average_predicted_success': statistics.mean([p.predicted_success_rate for p in self.import_predictions]) 
                                               if self.import_predictions else 0.0
                },
                'system_status': {
                    'historical_data_loaded': self.historical_data is not None,
                    'sender_patterns_analyzed': len(self.sender_patterns),
                    'prediction_confidence': 'high' if len(self.sender_patterns) > 10 else 'medium'
                }
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"Error generating predictive summary: {e}")
            return {'error': str(e), 'timestamp': datetime.now().isoformat()}

# Initialize Predictive Analytics for global use
predictive_analytics = PredictiveAnalytics()

def analyze_predictive_intelligence(contact_emails: List[str] = None, 
                                  forecast_days: int = 60) -> Dict[str, Any]:
    """Analyze predictive intelligence for contacts and imports"""
    logger.info(f"Running predictive intelligence analysis for {forecast_days} days")
    
    # Load historical data
    if not predictive_analytics.load_historical_data():
        logger.warning("Limited historical data available for predictions")
    
    results = {
        'timeline_predictions': [],
        'sender_forecasts': [],
        'business_insights': [],
        'import_predictions': [],
        'summary': {}
    }
    
    # Generate timeline predictions
    if contact_emails:
        for email in contact_emails:
            predictions = predictive_analytics.predict_timeline_gaps(email, forecast_days)
            results['timeline_predictions'].extend([asdict(p) for p in predictions])
    
    # Generate sender behavior forecasts
    if contact_emails:
        for email in contact_emails:
            forecast = predictive_analytics.forecast_sender_behavior(email, forecast_days)
            results['sender_forecasts'].append(asdict(forecast))
    
    # Generate business insights
    insights = predictive_analytics.generate_business_insights()
    results['business_insights'] = [asdict(i) for i in insights]
    
    # Generate import prediction example
    example_prediction = predictive_analytics.predict_import_success(100)
    results['import_predictions'].append(asdict(example_prediction))
    
    # Generate summary
    results['summary'] = predictive_analytics.get_predictive_summary()
    
    logger.info("Predictive intelligence analysis completed")
    return results

if __name__ == "__main__":
    # Example usage and testing
    logger.info("Predictive Analytics Engine - Phase 4.3 Module")
    logger.info("Testing predictive capabilities...")
    
    # Test predictive analytics
    test_contacts = ["test@example.com", "user@company.com"]
    
    results = analyze_predictive_intelligence(
        contact_emails=test_contacts,
        forecast_days=30
    )
    
    print("\n=== PREDICTIVE ANALYTICS RESULTS ===")
    print(f"Timeline predictions: {len(results['timeline_predictions'])}")
    print(f"Sender forecasts: {len(results['sender_forecasts'])}")
    print(f"Business insights: {len(results['business_insights'])}")
    print(f"Import predictions: {len(results['import_predictions'])}")
    
    print("\n=== PREDICTIVE SUMMARY ===")
    summary = results['summary']
    for category, data in summary.items():
        if isinstance(data, dict):
            print(f"{category.upper()}:")
            for key, value in data.items():
                print(f"  {key}: {value}")
        else:
            print(f"{category}: {data}")
    
    print("\nâœ… Predictive Analytics Engine Phase 4.3 testing completed successfully!") 